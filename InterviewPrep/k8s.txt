Architecture ===
Kubernetes Master ## all components runs as pod in kube-system namespace
            etcd <-> (key-value store that keeps the current state of cluster)
            kube-apiserver (serves REST operations, connects to etcd database)
            kube-controllermanager (Various Controllers that helps Managing cluster for desired state) Ex: Node/Replication/Endpoint Controllers
            kube-scheduler (schedules pods on specific nodes based on labels, taints, tolerations set for pods)
           
kubernetes node ## kube-proxy runs as pod(kube-system), kubelet is a service(systemctl)
            kubelet (Talks to container engine to ensure pods availability), 1. 
            kube-proxy (runs on everynode + uses iptables to provide an interface to connect to k8s components)
  
container runtime: takes care of actually running the containers. 
supervisord: monitors and guarantees the availability of kubelet and docker processes. 
network agent: implements a software defined n/w+ing solution, such as weave
logging: the CNCF project Fluentd used for unified logging in the cluster. a Fluentd agent must be installed on all k8s nodes. 

Controller types::
      Jobs - Supervises the pods running batch job, CronJobs - schedules the jobs. 
      Services - Allow communication b/w deployments
      Deployment - Gives declarative updates to pods/replicasets
      ReplicaSet - Ensure no.of replicas running all the time
      DaemonSet - Ensure that all nodes run a copy of a specific pod     
      
Kubelet service work:
      1. Communicate with API-server, find any pods assigned to nodes
      2. Do health check of pods/nodes, run pods via docker-engine
      3. mount volume/secrets
      >> kube-apiserver ==={podspecs(1,2,3)}===>> kubelet(ensures container health as described in podspec)
      
kube-proxy pod on each node work:
      1. Talk with API-server, add/remove services
      2. For each service, it opens a random port on local node
      3. traffic to that port are proxied to one of the corresponding backend pods. 
      
      
=================================================================================
Manifests: apiVersion,Kind,metadata,spec

volumes = name,emptyDir:{}/nfs/hostPath/persistentVolumeClaim/secret/configMap               
            ## nfs(server,path), hostPath(path), pvc(claimName), secret(secretName), cm(name) 
containers = name, image, ports(name,port), volumeMounts(name,path), env, lifecycle.{postStart,preStop}.exec.command, resources.{requests|limits}.{memory|cpu}
            ## env(name,ValueFrom.{secretKeyRef|configMapKeyRef}.key) ##

[Kind] = [spec] fields
Pod.spec = containers[0]...,volumes ##
Job.spec = template(pod.spec)
CronJob.spec = { ++schedule }

DaemonSet.spec =  selector.matchlabels, template
Deployment.spec = replicas, selector.matchlabels, template:pod.spec, serviceName            ## Deployment.spec >~ ReplicationSet.spec >~  ReplicationController.spec
StatefulSet.spec = { ++volumeClaimTemplate(metadata,spec) }

Service.spec = selector, type, port            ## service types = clusterIP,nodePort,LoadBalancer,ExternalName

persistentVolume.spec = nfs/hostPath, capacity.storage,  accessModes:RWO, storageClassName, 
pvClaim.spec =              resources.requests.storage, accessModes:RWO, storageClassName
secret.data = key1:<>,key2:<> ### secret.metadata.name = secret-name                        ### ex-keys: user: amar, pass: XXX  
configMap.data = key1:<file-content> ## keys=filenames

HorizontalPodAutoscaler.spec = scaleTargetRef(depl.spec), minReplicas, maxReplicas, metrics(resource=cpu/memory)

## kubeconfig for a user does requires .key + .crt => kubectl config set-credentials johndoe --client-key=johndoe.key --client-certificate=johndoe.crt --embed-certs=true
Create a Private key (.key file) ## using openssl command
CertificateSigningRequest.spec = request(<base64 .key file>) ## k certificate approve <csr.name> ==> .crt file-content 
Role.rules = apiGroups,resources,verbs            ## ex-resources=pods, verbs=get,list,create
RoleBinding.{subjects(User.name),roleRef(name)}
ClusterRole & ClusterRoleBinding in similar way does support serviceAccounts instead of a User. 

LimitRange spec = min/max/default - limit/requests - cpu/memory             ### per container
ResourceQuota spec = hard - limit/requests - cpu/memory, pods               ### per namespace(all pods)
            ## quota consumed using priorityClassName in pod.spec

=================================================================================

Cluster creation(kubeadm) ===
All nodes::
Install(yum) : docker(container runtime), kubelet(enable-service), kubeadm, kubectl
disable swap (free -m, /etc/fstab, disable swap mount)
disable firewall or open appropriate ports in the firewall
/etc/hosts, setup hostname resolving ##/etc/hosts # { IP Domain alias} ## ssh alias

On control: kubeadm init. (root), note down "kubeadm join" command after initialization.

client configuration file creation:(regular user)
 - create(~/.kube) && cp -l /etc/kubernetes/admin.conf ~/.kube/config && chown $(id -u):$(id -g) ~/.kube/config
 - kubectl cluster-info ; kubectl config view 
 
 Alternative: export KUBECONFIG=/etc/kubernetes/admin.conf
 kubectl config -h/view/get-context
 
 n/w add-on = kubectl apply -f <flannel/calico/Weave>.yaml ##  CNI(container n/w interface), pod communication
 Ensure namespace/kube-system - pod/{coredns,flannel} is running. If it is up and running, you can start joining nodes. 
 
--- Join nodes in cluster(root)
kubeadm join --token<> IP:6443 --discovery-token-ca-cert-hash <> ## if no token -> kubeadm token list/create/
=================================================================================

API and Objects ===
Install bash-completion && kubectl completion bash > /etc/bash_completion.d/kubectl

kubectl auth can-i create <pods/deployments> --as amusham --namespace <>
kubectl api-versions

Explain api components: 
kubectl explain pod.spec ## To list the fields pod.spec  ## pod.{metadata,kind,apiVersion,spec} # pod.spec.container # deployments.spe.strategy

Flow:-> Ingress -> service+LB -> deployments{upgrades+replication}(pod:ip+volume) -> PVC -> PV

How api works::: etcd <-> api-server <-> kubectl(~/.kube/config, includes TLS)
kubectl & alternates::: 
- kube-proxy -> curl(http) (here curl not takes care of TLS, so kube-proxy does) ## kubectl way
- direct curl(https) -> TLS, parse --cert, --key, --cacert values (Less important)
- kubectl proxy --port=8001 &

curl http://localhost:8001/version 
curl http://localhost:8001/api/v1/namespaces/default/pods/{<pod_name>}

sudo yum {provides */etcdctl, install -y etcd} && etcdctl -h (control/manage DB), 
etcdctl2 to talk with v2 API. 
=================================================================================

Deployments === 
Flow part: deployments{upgrades+replication}(pod:ip+volume)

kubectl {get, describe, create, scale(--replicas), edit, label}
get = (--show-labels, --selector) # Ex: label/selector app=name

kubectl create deployment --image=<> {} # Creates = (depl, rs, pod), depl-name={} 
Alt: kubectl create -f deplX.yaml
#1.deployments/{} 
#2.replicaset/{}-<10d-token> 
#3. pod/{}-<10d-token>-<5d-token> 

kubectl get deployment.apps nginxdepl -o yaml | less
  apiVersion: v1
  kind: Deployment
  metadata:{*name,namespace,labels}
  spec{replicas, selector.matchlabels, template:pod.spec, serviceName}
  
service.spec{selector, type, port} ## service types....
pod.spec.containers[0]{name, image, ports(name,port), volumeMounts(name,path), volumes(name,pvc/nfs)} 
statefulSet.spec{++volumeClaimTemplate(metadata,spec)}
  
Simple deployment : kubectl run --image=<> --replicas=<> ## get - depl/pod, describe - pod. 
Simple service: kubect expose deployment nginx --port=80 --name=myservice
=================================================================================

Storage, secrets, configMaps ===

simple storage - > 
    kind: pod
    metadata{*name, label}
    spec.Containers.volumeMounts: (name:test, mountPath: /centos1)
    spec.volumes: (name: test, emptyDir: {})

pod.spec.volumes{name, type} #types -> {PVC, ConfigMap, Secret}
---
PVC -> PV(NFS/Cloud storage)
      pod.spec{containers(volumeMounts), volumes(PVC.claimname)}
      pvc.spec{accessModes(rwm), resources(requests,limits)}
      pv.spec{capacity(storage), accessmodes(rwm), hostpath(path)/nfs(server/path)}
---
ConfigMap -> (files/dir/variables) , var: key=value
          1. kubectl create cm <cm-name> --from-file nginx.conf (or) --from-literal=user=amar --from-literal=pass=abc
For variables: 
          2. pod.spec.containers.env{name, valueFrom.configMapKeyRef(cm-name,key)} ## cm-name from kubectl command.          
For files: 
          2. pod.spec.volumes.{name, configMap.items.key} ## items.key is above cm-name.
          3. pod.spec.containers.volumeMounts{name,path} ## name is configMap from step2(defined volume)

---
Secrets -> (vars)(base64 encode, no-encrypt)
*** Types of secretes:  generic, TLS, docker-registry

          1. kubectl create secret generic secretstuff --from-literal=pass=abc --from-literal=user=amusham # tls{--key,--cert} 
          1. kubectl create secret docker-registry regcred --docker-server=<reg-server> --docker-username=<> --docker-password=<> --docker-email=<>
          kubectl get secret regcred --output=yaml  ##base64 decode data.dockerconfig
For tls:          
          2. ingress.spec{host, path, backend.serviceName, tls) ## ingress.spec.tls{host, secretName}
For docker-registry:
          2. pod.spec.imagePullSecrets.name = regcred ## regcred from kubectl command 
For variables: 
          2. pod.spec.containers.env{name, valueFrom.secretKeyRef(cm-name,key)} ## cm-name from kubectl command.  
For generic:
          2. pod.spec.volumes{name, secret.secretName}
          3. pod.spec.containers.volumeMounts{name, mountPath} # name is secret from step2.
=================================================================================

Services===
Each pod has privateIP address. service allow apps to receive traffic, routes accross set of pods. 
It allows pods to die and replicate in Kubernetes without impacting your application. It monitors the running pods using endpoints, to ensure traffic goes to only available pods

Service Types:: service can be exposed in different ways

ClusterIP (default) - Exposes on internalIP of cluster. It makes the app reachable from within the cluster.
NodePort - Exposes on same port of each Node using NAT. It makes app accessible outside the cluster(<NodeIP>:<NodePort>)
LoadBalancer - Creates an external load balancer in the current cloud and assigns a fixed, external IP to the Service.
ExternalName(with CNAME) - Exposes the Service using an arbitrary name (specified by externalName in the spec). This type requires v1.7 or higher of kube-dns.



=================================================================================

Commands===
On resources - get(list), describe(details), logs, exec(command) 
kubectl exec -ti $POD_NAME -- /bin/bash
kubectl exec -ti $POD_NAME curl localhost:8080  

kubectl get {pods, nodes} -n <> --show-labels --selector env=prod
kubectl delete pods -l env!=prod

kubectl expose deployments/helloworld --type="NodePort" --port 8080
kubectl {get, describe} service/helloworld -o yaml 
kubectl {get, delete} services -l app=name

kubectl scale deployments/helloworld --replicas=4 ## get {deployment,rs,pods} & describe deployments/helloworld
Kind: HorizontalPodAutoscaler
kubectl -n <> get hpa ## Ex: app-hpa, reference=statefulset/app, targets=cpu,memory. maxpods/minpods/replicas. 
kubectl -n <> describe hpa app-hpa 

Rolling update:
kubectl set image deployments/helloworld helloworld=<image>:v2 ## describe pods, find out image used.
kubectl get {deployoments, rs, pods} && kubectl describe deployment helloworld
kubectl describe services/helloworld ## To check exposed IP and Port
kubectl rollout status deployments/helloworld ## successful 
kubectl set image deployments/helloworld helloworld=<image>:v10
kubectl rollout history deployment/helloworld
kubectl rollout undo deployments/helloworld ## rollout to previous state i.e., v2 

kubectl get {jobs, cronjobs} && kubectl edit cronjobs/<>

$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
$ export NODE_PORT=$(kubectl get services/helloworld -o go-template='{{(index .spec.ports 0).nodePort}}')

=================================================================================

architecture-work  - https://www.edureka.co/blog/kubernetes-architecture/

deployment/replicaset-work

who deletes the app:v1 after upgrade to app:v2 - Ans: delete the deployment/rs when ingress is pointed 100% to app:v2. 
Ingress.metadata.annotations - nginx.ingress.kubernetes.io/canary=true, canary-weight=10 ## % of traffic. - canary-ingress.yaml
1. kubectl apply -f app-v1.yaml & ingress-v1.yaml
2. kubectl apply -f app-v2.yaml & canary-ingress.yaml
3. kubectl delete -f canary-ingress.yaml ; kubectl apply -f ingress-v2.yaml

Deployment strategies. 
https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/canary

https://www.cncf.io/wp-content/uploads/2020/08/CNCF-Presentation-Template-K8s-Deployment.pdf


How pvc and pv connected?
ingress?
statefulSet - https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/ ## ex-usecase = databases
            1. StatefulSet provides unique identitiy for each pod(web-0,web-1,web-2). Pods are deleted/created in order. 
            2. Each pod gets its own dedicated persistent storage. When pod deleted and recreated, it will reattach to its original PVC
            3. It ensures order of scaling or deployment. Also scaled up or down in Order. Also, rolling updates
DaemonSets(runs on each node) - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
deployment strategy = 1.rolling update???? 2.canary
HorizontalPodAutoscaler

Add services

Prepare intro, skill set, role in team/skill
